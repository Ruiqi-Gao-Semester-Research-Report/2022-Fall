%%%
%% This is file `sample-sigplan.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigplan')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigplan.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%\documentclass[sigplan,nonacm]{acmart}
\documentclass[sigplan, nonacm]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

\graphicspath{{pictures/}}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
	\providecommand\BibTeX{{%
			\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%%\setcopyright{acmcopyright}
\setcopyright{none}
%%\acmJournal{PACMPL}
%%\acmVolume{1}
%%\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
%%\acmArticle{1}
%%\acmYear{2021}
%%\acmMonth{9}
%%\acmDOI{}
%%\copyrightyear{2018}
%%\acmYear{2018}
%%\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
%%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium%% on Neural
%%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
%%\acmPrice{15.00}
%%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}
\sloppy

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Parallel training by Pipeline Execution In AIRCoP}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ruiqi Gao}
\email{gao606@purdue.edu}
\affiliation{%
  \institution{Purdue University}
  \city{Lafayette}
  \state{Indiana}
  \country{USA}
  \postcode{47901}
}

\begin{abstract}
  Various approaches have been developed like data parallelism, model parallelism, attribute parallelism, and pipeline parallelism to train machine learning models in parallel. However, there are no existing frameworks that can integrate these approaches. Most commonly used machine learning frameworks such as TensorFlow and Pytorch only support a tiny subset of parallel paradigms - the data parallelism and model parallelism. GPipe and PipeDream support different types of pipeline parallelism by dividing the model into several modules and running them in a pipeline fashion.
  AIRCop (Abstract IR for Comprehensive Parallelization) is a developing project that unifies all the parallelism paradigms mentioned above. Since all the machine learning models can be viewed as computation graphs, the parallelism paradigms (data parallelism, model parallelism, attribute parallelism, and pipeline parallelism) are just different kinds of computation graph transformation, i.e., transformation pass. AIRCop uses this idea and takes a machine learning model (basic sequential model) and parallelization annotations (describe the desired parallel behavior) as input and generates target program through different transformation passes.

\end{abstract}

\keywords{Parallel Training, Graph Transformation, Machine Learning Frameworks, Pipeline Parallelism}

\maketitle
\section{Proposal Retrospect and Progress}
\subsection{Proposal Retrospect}
In the proposal, we mentioned that this project would support both stacked pipeline and queued pipeline. However, we currently only support stacked pipeline for pipeline execution.\par
The reason is that we spent some time adding support for multiple-module for AIRCoP because AIRCoP initially only supported single module. This work involves changing the syntax of the frontend to make writing multiple-module models intuitive and easy. After finishing support for the stacked pipeline, We also found a severe problem in AIRCoP that will cause significant performance overhead - Memory Management. AIRCoP initially uses cudaMalloc and cudaFree inside the training loop for memory allocation. Due to the complex synchronization mechanism of cudaMalloc/Free, these operations will cause a very long time. After analyzing the running time with NVIDIA profiler, we found that most of the execution time is spent on cudaMalloc/Free, and it could be even 10x faster by pre-allocating the memory before the training loop. So we focus on addressing the memory management problem after finishing stacked pipeline. We currently use some code-motion techniques that will lift all the cudaMalloc out of the training loop for optimization.\par
Because queued pipeline parallelism uses asynchronous weight update and will use lagged network weight, we choose to address the memory management problem first due to limited time.\par
Because we only currently implement stacked pipeline, all the pipeline parallelism used in this paper refers to stacked pipeline parallelism.
\subsection{Progress}
Although we only support stacked pipeline, it is sufficient for multi-module pipeline parallelism. We can still mix spatial partition with pipeline execution to explore different parallelism settings and gain insight into parallel training, as we will see in the evaluation.\par
Our progress is shown as follows:
\begin{itemize}
  \item We formalize the transformation procedure of stacked pipeline as training loop transformation on the formal language of AIRCoP.
  \item We implemented stacked pipeline for AIRCoP.
  \item We optimize the memory management for AIRCoP.
  \item We run evaluation benchmarks with different parallelism settings(Spatial Partition, Spatial Partition mixed with Pipeline Execution) to validate our implementation and gain some insight into parallel training.
\end{itemize} \par
In section \ref{introduction}, we introduce parallel training, different existing parallelism strategies, and AIRCoP.\par
In section \ref{pipelineparallism}, we further study stacked pipeline - the pipeline parallelism we support in AIRCoP.\par
In section \ref{formalpipe}, we show the formal description of the training loop transformation for pipeline parallelism.\par
In section \ref{implementation}, we present the implementation detail of our work.\par
In section \ref{evaluation}, we present a parallel training evaluation with different parallelism settings to illustrate the flexibility of our framework.\par
In section \ref{conclusion}, we summarize our progress and draw some conclusions.
\section{Introduction} \label{introduction}
 Real-world machine learning applications are usually tremendous. They require a tremendous amount of data for training and can not work on a single device. As a result, various parallel strategies have been proposed for running large models in parallel among multiple distributed devices to achieve optimal performance.\par
 \subsection{Spatial Partition}There are different kinds of parallelism strategies, the most common one is Data parallelism\cite{krizhevsky2012imagenet}, where we split input across multiple devices and maintain the same network weight. Data parallelism is adapted and developed by most modern machine learning framework (TensorFlow\cite{abadi2016tensorflow}, Pytorch\cite{paszke2019pytorch}). Data parallelism requires network weight synchronizations between devices at the end of each iteration. Therefore Data parallelism is ideal for a computation-intensive network with few weight parameters (convolution and pooling layer) but will introduce significant communication overhead for a network with a large number of weight parameters (fully-connected layer). Model parallelism\cite{dean2012large} is achieved by splitting the network weight tensor across devices while broadcasting the input tensor. Model parallelism reduces the communication overhead of Data Parallelism but is rarely used because there are often dependencies between network layers. Attribute parallelism\cite{jia2019beyond} can partition the attribute dimension in both input and weight parameters and require collective communication like AllReduce between operations. An experienced developer can combine different Parallelism strategies to achieve optimal performance. For example, \cite{krizhevsky2014one} uses data parallelism for the convolution layer and model parallelism for the fully-connected layer. The above parallelism strategies can be categorized as Spatial Partition because they are based on the partition for data/weight/operation. \par
 \subsection{Pipeline Parallelism}
 Unlike Spatial Partition, where input data/weight is distributed across multiple devices, there is another parallelism strategy: Pipeline Parallelism. Pipeline Parallelism divides a machine learning model into several modules with data dependency between them, divides a batch into several micro-batches, and runs them in parallel in a pipelined fashion. Both GPipe\cite{huang2019gpipe} and Pipedream\cite{narayanan2019pipedream} studied and developed Pipeline parallelism. Pipeline parallelism is especially useful when the training system has low-bandwidth connections because it can limit the communication overhead to only happen at module boundaries.\par
 As shown in \cite{krizhevsky2014one}, combining different parallelism strategies can achieve better performance for different applications. However, no existing machine learning framework can support all the parallelism strategies. TensorFlow\cite{abadi2016tensorflow} and Pytorch\cite{paszke2019pytorch} only supports Data Parallelism while GPipe\cite{huang2019gpipe} and Pipedream\cite{narayanan2019pipedream} supports Pipeline parallelism. Users are limited to only a subset of Parallelism strategies and can not fully utilize the distributed training system.\par
  Different distributed system topologies also would require different strategies for optimal performance. As we mentioned before, Spatial Partition may introduce frequent tensor communication overhead between operations.
 \begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.7]{networktopo.png}
  \caption{Large training system}
  \label{fig:networktopo}
\end{figure} This overhead may be negligible for a GPU Cluster with high-bandwidth interconnections but will cause a significant performance drop for large distributed systems consisting of several nodes of GPU clusters with low bandwidth connection between them as shown in Figure \ref{fig:networktopo}. Because high bandwidth inter-GPU connection techniques like NV-Link can only scale to a limited number of devices, large training servers are often deployed across multiple nodes. It will be ideal to divide the model into several modules, assign each module to a cluster, use Spatial partition inside a GPU cluster, and adopt pipeline parallelism between modules.\par
\subsection{AIRCoP}
 AIRCoP \cite{aircopfeiwang} is a machine learning framework that supports all the parallel parallelisms above, therefore allows users to fully explore different parallel settings. AIRCoP implemented spatial partition before and our work implemented Pipeline parallelism for AIRCoP, thus make AIRCoP a unifying parallel training framework.\par
 %\begin{figure}[htbp]
 % \centering
 % \includegraphics[scale=0.7]{FormalSyntax}
 % \caption{Formal Syntax of AIRCoP}
 % \label{fig:sample}
%\end{figure}
\begin{align*}\label{formalsyntax}
  \textbf{TensorType}:& \\
  s \in Symbol& \quad n  \in \mathbb{N} \\
  size \in Size& :=\mathcal{S}(dim:Symbol,length:\mathbb{N})\\
  tt\in TensorType& := \mathcal{T}(shape:List[Size])\\
  \textbf{OperationType}:& \\
  mem\_op\in MemoryOp& := Weight \text{ } | \text{ } Input \text{ } | \text{ } Zero \text{ } | \text{ } One \text{ } \dots \\
  ml\_op \in MLOP& := MatMul \text{ } | \text{ } Conv \text{ } | \text{ } AvgPool \text{ }\text{ } | \dots \\
  op \in OperationType&:= mem\_op \text{ } | \text{ } ml\_op \\
\end{align*}
\begin{align*}
  \textbf{Module}:& \\
  x \in Var& \quad \quad \quad xs \in List[Var]\\
  operation \in Operation&:= xs \text{ } | \text{ } op(inputs: List[Var], \\
  & out\_types:List[TensorType]) \\
  module \in Module&:=xs \text{ } | \text{ } \textbf{let } xs=operation \textbf{ in } module \\
  \textbf{Model}:&\\
  model \in Model&:= \quad List[Module]
\end{align*}
 AIRCoP developed a formal syntax to represent tensor computation graphs. The syntax mainly has three components: Tensor and corresponding TensorType to represent input tensor and weight, Operation, and corresponding Operation Type to represent different Machine learning operations like Conv/MatMul/Relu and their input/output Type, Module to represent different modules. Operations are connected through let-bindings. Furthermore, we use annotation of the formal syntax to represent different parallelism strategies formally. We developed annotation systems for both Tensor, Operation, and Modules. Tensor and Operation annotation represents Spatial Partition by indicating the dimension to partition. Module annotation indicates how many micro-batches the mini-batch is split into. Users only need to write the sequential forward computation of the Machine Learning model, and the correct final training loop will be automatically computed through a series of transformations.
 \section{Pipeline Parallelism} \label{pipelineparallism}
 Pipeline Parallelism is ideal for distributed training workload across several nodes with weak inter-connection. Using Spatial Partition across different nodes will cause significant communication overhead between each operation. Splitting the training models into several modules and assigning each module to different nodes will limit the inter-node communication to the boundary of each module, therefore significantly reducing the inter-node communication overhead.\par
 \begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.4]{NormalPipeline}
  \caption{Naive multiple modules}
  \label{fig:multiplemodule}
\end{figure}
 Simply splitting modules will cause large amount of stall time (or bubble time) due to complex dependencies between the sequential modules. As shown in Figure \ref{fig:multiplemodule}, if we naively spilt the model into 4 modules $m_0,m_1,m_2,m_3$. There will be sequential dependencies between the forward pass $F_0, F_1, F_2, F_3$ of modules $m_0,m_1,m_2,m_3$, and reverse dependencies between backward pass $B_3, B_2, B_1, B_0$. The dependencies between the forward pass and backward pass will force strict ordering between modules, introduce synchronizations at module boundaries, and cause significant idle time on 4 devices. \par
 \begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.45]{stackedpipeline}
  \caption{Pipeline Parallelism}
  \label{fig:stackedpipeline}
\end{figure}
\subsection{Stacked Pipeline Parallelism}
 Our approach is similar to GPipe\cite{huang2019gpipe}. We split a batch into multiple micro-batches and sequentially run the forward pass and the backward pass of multiple micro-batches. Because different micro-batches are stacked, we also call this Stacked Pipeline Parallelism. Only the forward and backward passes of the same micro-batches have dependencies between them. Different modules of different micro-batches could run in parallel as shown in Figure \ref{fig:stackedpipeline}. Stacked Pipeline parallelism allows users to fully utilize distributed devices by minimizing stall time caused by cross-module dependencies. \par
 Stacked Pipeline parallelism will insert blocking send/recv operations at the module boundaries to maintain necessary data dependencies. The gradient of multiple micro-batches will be accumulated and is used to update weight synchronously for the entire mini-batch. Stacked Pipeline parallelism can easily scale to a large number of devices while maintaining ideal utilization of resources.\par
 Stacked Pipeline parallelism will introduce little communication overhead because it will only transfer data at the module boundaries rather than the frequent inter-operation tensor communication introduced by Spatial Partition. This feature of Stacked Pipeline parallelism makes it suitable for both high-speed interconnection systems and weakly connected systems. \par
 Throughout the rest of this paper, the term Pipeline parallelism refers to Stacked Pipeline parallelism.
 \subsection{Pipeline Parallelism in AIRCoP}
 Pipeline parallelism in AIRCoP is done through module annotation systems. The annotation consists of an integer representing the number of micro-batches (or pipelines).
 Users can split the model into several modules and annotate them for Pipeline parallelism. The resulting training loop is different from naive multiple modules.
 \begin{align*}
  \textbf{ModuleAnnotation}: \qquad \qquad& \\
  a \in PipeAnno  := \mathcal{K} (\epsilon : \mathcal{N})\qquad \qquad&
\end{align*}
Assume we have $\epsilon$ pipelines. The target program will have a top-level training loop for iterating mini-batches. Inside each mini-batch iteration, there will be a training loop of size $\epsilon$ for iterating the forward passes of the $\epsilon$ micro-batches and a training loop of size $\epsilon$ for iterating the backward passes of the $\epsilon$ micro-batches. The gradient will be accumulated inside each backward pass of micro-batches. After the backward pass training loop finishes, the weight will be updated based on gradient descent for the entire mini-batch.
 \section{Formal description of Pipeline Parallelism Transformation}\label{formalpipe}
%\begin{align*}
%  \textbf{ModuleAnnotation}:& \\
%  a \in PipeAnno & := \mathcal{K} (\epsilon : \mathcal{N}) \\
%  \textbf{Model}:&\\
%   s\_model \in Model&:= \quad List[Module \textbf{ by } PipeAnno] \\
%   \textbf{Transformation}:&\\
%   Transform&:Model \rightarrow Model \\
%   Transform (module::modulelist)&\triangleq Transform(module)::Transform(modulelist)\\
%   Transform (module \in Module)&\triangleq Collection(module)(\emptyset,\emptyset,\emptyset)\\
%   \textbf{Collection Phase}:&\\
%   Collection: Module \rightarrow List[Var \times Operation]& \rightarrow List[Var \times Operation] \rightarrow List[Var \times operation] \rightarrow Module \\
%   Collection(\textbf{let } xs = &Weight([], ots) \textbf{ in } module)(E_w,E_f,E_b)\triangleq \\
%   Collection&(module)(E_w :: (xs, Weight([], ots), E_f,E_b) \\
%   Collection(\textbf{let } xs = &Input([], ots) \textbf{ in } module)(E_w,E_f,E_b)\triangleq \\
%   Collection&(module)(E_w, E_f :: (xs, Input([], ots)),E_b) \\
%   Collection(\textbf{let } xs = &ml\_op(ins, ots) \textbf{ in } module)(E_w,E_f,E_b)\triangleq \\
%   Collection(module)(E_w, &E_f :: (xs, ml\_op(ins, ots)),(ins^\prime, ml\_op^\prime(ins::xs::xs^\prime, ins.tt))::E_b) \\
%   Collection(xs)&(E_w,E_f,E_b) \quad if \text{ (xs is not used in subsequent module)}\triangleq\\
%   Generation(E_w, &E_f,(xs^\prime, One([], xs.tt))::E_b) \\
%   Collection(xs)&(E_w,E_f,E_b) \quad if \text{ (xs is used in subsequent module)}\triangleq\\
%   Generation(E_w, &E_f::([], Send([xs],xs.tt)),(xs^\prime, Recv([], xs.tt))::E_b) \\
%   \textbf{Generation Phase}:&\\
%\end{align*}
Transformation for pipeline parallelism has 2 stages; \textbf{Collection Phase} and \textbf{Generation Phase}. The Collection Phase collects the necessary information like forward/backward operation. The Generation Phase uses the collected information and the module annotation to generate the training loop.\par
\paragraph{Collection Phase}
During Collection Phase we need to collect the weight operations $E_w$, forward operations $E_f$ and backward operations $E_b$. For pipeline parallelism, we will add send/recv into $E_f$ and $E_b$ to maintain the data dependencies between modules. The corresponding variables of the operation is $xs^w$, $xs^f$, $xs^b$. We also need to calculate the lift variables $$xs^l = Defined(E_f) \bigcap Used(E_b)$$, the lifted variables $xs^l$ are the variables defined in the forward pass and used in the backward pass. Because we are generating different loops for forward passes and backward passes of micro-batches, we need to lift $xs^l$ into the outer scope and keep multiple versions of those variables for different micro-batches.\par
\paragraph{Generation Phase}
The formal description of the Generation Phase is as follows:
\begin{align*}
  \textbf{Generation Phase}: \qquad \qquad& \\
  G(E_w,E_f,E_b, xs^w,xs^f,xs^b,xs^l,\alpha:\mathcal{K} (\epsilon))& \triangleq \\
  G_w(E_w)G_l(xs^l)(loop \text{  } i=[0\dots n/\epsilon] &\textbf{ in}\\
  loop \text{  }j=[0\dots \epsilon]\text{ }G_f(E_f, xs^l) \textbf{ then}&\\
  loop \text{  }j=[0\dots \epsilon]\text{ }G_b(E_b, xs^f) \textbf{ then}&\\
  G_o(xs^w) \textbf{ then } xs^w\quad \qquad \qquad&
\end{align*}
\begin{align*}
  G_w&((xs, Weight([], out\_type))::E_w)\triangleq\\
  &\textbf{let } xs=Weight([], out\_type) \textbf{ in}\\
  &\textbf{let } xs^\prime=Zero([], out\_type) \textbf{ in}\\
  &\textbf{let } xs^m=Zero([], out\_type) \textbf{ in}\\
  &G_w(E_w)\\
  G_l&(xs::xs^l)\triangleq\\
  &\textbf{let } xs^\epsilon=Zero^\epsilon([], xs.type) \textbf{ in}\\
  &G_l(xs^l)\\
  G_f&((xs, op([input], out\_type))::E_f, xs^l)\triangleq\\
  &\textbf{let } xs=op([input], out\_type) \textbf{ in}\\
  &G_f(E^f, xs^l)\\
  G_f&(\emptyset, xs::xs^l)\triangleq\\
  &\textbf{let } \phi=Assign([xs, xs^\epsilon], xs.type) \textbf{ in}\\
  &G_f(\emptyset, xs^l)\\
  G_b&(E_b, xs::xs^f)\triangleq\\
  &\textbf{let } xs^\prime=Zero([], out\_type) \textbf{ in}\\
  &G_b(E^b, xs^f)\\
  G_b&((xs^\prime, op^\prime([input], out\_type))::E_b, \emptyset)\triangleq\\
  &\textbf{let } t=op^\prime_l([input], out\_type) \textbf{ in}\\
  &\textbf{let } \phi=Accumulate([t, xs^\prime], out\_type) \textbf{ in}\\
  &G_b(E^b, \emptyset)\\
  G_o&(xs::xs^w)\triangleq\\
  &\textbf{let } \phi=Optimize([xs,xs^\prime,xs^m], xs.type) \textbf{ in}\\
  &G_o(xs^w)\\
  \\
  op_l&([input],out\_type) \triangleq op([input^\epsilon], out\_type)
\end{align*}
$op_l$ means that the operation will read from the versioned lift variable instead of the original variable. $xs^\epsilon = Zero^\epsilon$ will generates an array of $\epsilon$ versions of the variable $xs$.\par
$G_w$ generates the weight and their gradient and momentum, $G_l$ generate an array of lift nodes for different micro-batches, $G_f$ generates forward pass and write the correct value to the versioned lift variables, $G_b$ generates the backward pass and read from the versioned lift variable, $G_o$ generates the optimization pass. Because our source language does not allow mutation, all the mutation operations (Assign/Accumulate/Optimize) are automatically generated in the Generation Phase. We only need to write the versioned lift node at the end of the forward pass, and the Generation phase can generate the correct training loop.
\section{Implementation} \label{implementation}
We implement AIRCoP and Stacked Pipeline Parallelism in LMS compiler framework\cite{rompf2010lightweight}, and generate CUDA\cite{sanders2010cuda} code with MPI\cite{gropp1999using}/NCCL for communication as the target executable program.\par
\subsection{Integration into LMS}
 LMS (Lightweight Modular Staging) is a library-based staging compiler that distinguishes stages solely based on Type information. LMS is beneficial for developing DSL through overloading. It has a typed frontend making the code generation safe. LMS offers lifted (next-stage) versions of regular operators and standard control-flow statements, making symbolic code generation easy and convenient. AIRCoP is simply a tensor computation DSL embedded inside LMS framework.\par
We embedded the formal syntax of AIRCoP inside the IR of LMS. LMS has ANF\cite{flanagan1993essence} form sea of Nodes\cite{click1995simple} IR. It also has an effect system for dealing with side effects and dependencies introduced by effects. LMS can perform Dead Code Elimination and Common Subexpression Elimination, making it sufficient for our tensor computation graph transformation. LMS IR is customizable and offers users the flexibility to define their operation while maintaining the data and effect dependencies between them. We use custom IR nodes to represent Input/Weight Tensor and Machine learning Operations and use the effect system to represent their dependencies. The Module can be simply represented by a Node with a block that contains all of its operations. \par
The typed frontend source language of AIRCoP only contains the sequential forward computation of the model and annotations. The source language does not mutate tensors. We use overloading of the typed frontend objects and operations and reflection to generate the corresponding IR of the source program. We put the explicit TensorType and Annotations into the RHS of the IR to make it persistent and used in later transformation. \par
\subsection{Training loop transformation For Pipeline Parallelism}
We use the transformation pass of LMS to transform the IR and simulate the tensor graph transformation to generate the correct training loop. We first need to perform DimName pass to merge the equivalent dimension of different tensors. For example, if matrix multiplication happens between matrixes of dimension $[m, n]$ and dimension $[j,k]$, then dimension $n$ and dimension $j$ can be merged.  Annotations of tensors and operations represent which dimension to partition, so we have to update those annotations too. \par
The next pass DistributeTensorAIRCoP is the core pass to implement training loop transformation. This pass has 2 phases \textbf{Collection Phase} and \textbf{Generation Phase}.\par
\paragraph{Collection Phase}
The Collection Phase collects the forward operations, the weight nodes, and the corresponding backward operation of each node. The backward operation is collected in reverse order to maintain the correct semantic of differentiation. Because the backward pass should be generated after the forward pass, we collect backward operation in the form of closures. The Collection Phase will also collect paired send/recv operations at the boundaries of different modules to maintain the correct data dependencies. We implement the Collection Phase by defining a function aircopCollect for each IR Node to collect the corresponding information. After the Collection Phase, we have the necessary information to perform automatic differentiation (AD). We also need to collect lifted tensors to generate the correct training loop. As we mentioned before, the forward pass and backward pass of micro-batches are iterated in different loops to achieve pipeline parallelism. Therefore, the tensors allocated in the forward pass and used in the backward pass will cause an error because forward and backward passes are now in different scopes. These tensors are called lifted tensors. We have to lift them into the outer scope and maintain multiple versions of them as an array of tensors. Micro-batches' forward and backward passes will use different versions of the lifted tensors. After collecting the lifted tensors, the Collection Phase is finished.\par
\paragraph{Generation Phase}
The Generation Phase generates the correct training loop based on the collected information. Inside each module, it will first generate the top-level training loop for mini-batches. Then it will generate the forward training loop and backward training loop for micro-batches based on Module Annotations and use the collected backward operations to realize automatic differentiation as we mentioned in Section \ref{formalpipe}. Inside both the forward and backward pass, the read and write to the lifted tensors will be transformed into read/write to the versioned tensor array. The weight gradient will be accumulated in the backward pass and optimized at the end of the entire mini-batch.\par
\subsection{Spatial Partition Pass}
The next pass will perform the Spatial Partition transformation. This part is of past work, so we will not dig further into this pass. The main purpose of this pass is to partition the tensor and operations based on annotations and add the corresponding inter operation tensor communication for synchronization (AllReduce for MatrixMultiplication or Convolution).\par
\subsection{MPI/NCCL Pass}
The final pass is to generate the corresponding MPI/NCCL instructions for inter-device communications. NCCL (NVIDIA Collective Communications Library) is the communication library introduced by NVIDIA for efficient collective and point-to-point communication across multiple GPU and multiple Nodes. Our target program is CUDA code with MPI/NCCL for communication. We use MPI to set up the environment and the NCCL communicator. We will create 2 NCCL communicators, one global communicator for cross-module communication (NCCLSend/Recv) and one local communicator for collective tensor communication (NCCLAllReduce) inside modules. We also need to handle tensor communication between modules if they have different Spatial partitions, which can be naively implemented by gather and scatter. We use cuBLAS and cuDNN for matrix computation and convolution networks.\par
We statically allocate GPU memory before the training loop and avoid time-exhausting cudaMalloc/cudaFree inside the training loop to optimize memory management. We currently use code-motion techniques to lift the cudaMalloc inside the training loop and pre-allocate the memory before the training loop to optimize memory management. This approach could significantly improve performance because cudaMalloc/Free often takes a very long time.\par
Our generated CUDA code can efficiently represent the correct training loop of machine learning models with both Spatial Partition and Pipeline Parallelism.
 \section{Evaluation}\label{evaluation}
 This section shows the evaluation benchmarks of various forms of parallel training by running our generated CUDA code on 4 GeForce TITAN XP GPUs on server cuda.cs.purdue.edu. The server is a single GPU cluster with a high-speed inter-GPU connection. The ideal network topology for Pipeline parallelism is an extensive training system consisting of several Nodes, but we currently could not run the evaluation under that scenario due to limited hardware resources and time. The purpose of this benchmark is to validate our graph transformation and gain a deeper insight into the choice of different parallelism strategies.\par
 We perform our test on a 12 layer convolution network for 40 iterations with different parallelism settings:
 \begin{itemize}
   \item Data parallelism running on 2 GPUs
   \item Data parallelism running on 4 GPUs
   \item Mixed parallelism running on 4 GPUs --- Naively partitioned into 2 modules with each module running in Data parallelism on 2 GPUs
   \item Mixed parallelism running on 4 GPUs --- 2 modules in Stacked Pipeline parallelism (4 pipelines) with each module running in Data parallelism on 2 GPUs
   \item Mixed parallelism running on 4 GPUs --- 2 modules in Stacked Pipeline parallelism (8 pipelines) with each module running in Data parallelism on 2 GPUs
 \end{itemize}
 \begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{pipelineparallelruntime.png}
  \caption{Data Parallelism vs Pipeline parallelism}
  \label{fig:runtime}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.5]{pipelineparallelspeedup.png}
  \caption{Data Parallelism vs Pipeline parallelism}
  \label{fig:speedup}
\end{figure}
 The running time of 5 settings is shown in Figure \ref{fig:runtime}, the corresponding speedup is shown in Figure \ref{fig:speedup}. We can notice that simply dividing a model into 2 modules will not cause any speed up due to the low device usage caused by the cross-module dependencies. The device will idle while waiting for data to arrive. We can notice that using Pipeline parallelism with 4 pipelines rather than naive module partition on 2 modules will cause a speedup of 1.6x. Moreover, we can further accelerate the training loop by increasing the pipeline number (micro-batch number) to 8 and reaching a speedup of 1.77x. However, keep in mind that increasing pipeline number will increase memory usage due to versioned tensor.\par
 The speedup achieved is significant and is very close to the theoretical limit. No matter how many pipelines we choose, some bubble time will still exist because the different modules of the same micro-batch will still have dependencies between them. For 2 modules, the theoretical speedup limit is $\frac{2n}{n+1}$ ($n$ is the pipeline number), which is about the same as our evaluation result.\par
 However, we find out that 2 modules on Stacked Pipeline parallelism with each module running at Data Parallelism on 2 GPUs (Mixed Parallelism) still perform a little worse than Data parallelism on 4 GPUs. This result is anticipated because we perform our evaluation on a single GPU cluster with very high-speed inter-GPU connections, which makes the frequent tensor communication (ALLReduce) overhead introduced by Data Parallelism negligible. However, the cross-module dependencies for micro-batch will still cause some bubble time. As we mentioned before, the ideal network topologies for Pipeline parallelism are an extensive training system consisting of multiple GPU clusters connected through the network. Then we can use Data parallelism inside each cluster and Pipeline parallelism between clusters. We currently could not perform evaluation under this ideal scenario due to limited time and hardware resources, but we could predict that Pipeline parallelism mixed with Data parallelism will perform much better than only using Data parallelism in this scenario because Pipeline parallelism could reduce the communication overhead because inter-cluster communication only happens at module boundaries.\par
 The evaluation shows that our framework is flexible, and the user could switch to different combinations of Parallelism settings to reduce the total communication overhead and achieve optimal performance. It also demonstrates that our framework can unify both Spatial Partition and Pipeline Parallelism, validates the correctness of our training loop transformation, and offers a deeper insight into how to choose between different parallel training settings.
 \section{Conclusion} \label{conclusion}
In conclusion, we implement Pipeline parallelism for AIRCoP, make it a machine learning framework that can integrate different parallelism strategies, and offer user flexibility to fully explore different combinations of parallel training strategies under different hardware scenarios and gain a deeper insight into parallel training.
\bibliographystyle{acm}
\bibliography{paper}
\end{document}
\endinput
